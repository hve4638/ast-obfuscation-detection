{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv, json\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 1024\n",
    "STEP_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(byte_stream:np.ndarray):\n",
    "    total_count = np.sum(byte_stream)\n",
    "\n",
    "    P_X = byte_stream / total_count\n",
    "\n",
    "    P_X_nonzero = P_X[P_X > 0]\n",
    "    H_X = -np.sum(P_X_nonzero * np.log2(P_X_nonzero))\n",
    "\n",
    "    P_H_given_X = P_X * H_X\n",
    "    P_H_X = P_X * P_H_given_X\n",
    "\n",
    "    return P_H_X\n",
    "\n",
    "# deprecated\n",
    "def get_byte_entropy(byte_array:np.ndarray):\n",
    "    return calculate_entropy(byte_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 16\n",
    "\n",
    "def export_image(filename:str, pixel1d:np.ndarray, size:int=SIZE):\n",
    "    pixel1d_int = np.round(pixel1d, 0).tolist()\n",
    "    pixel2d = [pixel1d_int[i * size:(i + 1) * size] for i in range(size)]\n",
    "    try:\n",
    "        final_image = Image.fromarray(np.array(pixel2d, dtype=np.uint8), 'L')\n",
    "        final_image.save(filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while saving image '{filename}'\")\n",
    "        print(e)\n",
    "        print(pixel1d)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_minmax(arr:np.ndarray)->np.ndarray:\n",
    "    min_val = np.min(arr)\n",
    "    max_val = np.max(arr)\n",
    "    if min_val == max_val:\n",
    "        return np.zeros_like(arr)\n",
    "    else:\n",
    "        return (arr - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dirpath(exportdir):\n",
    "    os.makedirs(exportdir, exist_ok=True)\n",
    "\n",
    "def ensure_filepath(exportfile):\n",
    "    dirpath, _ = os.path.split(exportfile)\n",
    "    if dirpath:\n",
    "        os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "def opencsv(filename:str, mode:str='r'):\n",
    "    return open(filename, mode, encoding='utf-8', newline='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, proc_bytestream:callable, *, on_normalize=None, export_csvfile, export_directory='./export'):\n",
    "    '''\n",
    "    proc_bytestream: (byte_stream:List[int]) -> List[int] : 들어오는 바이트스르림 처리\n",
    "    on_normalize: (data:List[int]) -> List[int] : 처리된 데이터를 정규화 수행\n",
    "    '''\n",
    "    ensure_dirpath(export_directory)\n",
    "    ensure_filepath(export_csvfile)\n",
    "\n",
    "    if on_normalize is None:\n",
    "        on_normalize = lambda x: x\n",
    "    \n",
    "    with opencsv(export_csvfile, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['index', 'path', 'filename', 'bytes'])\n",
    "        \n",
    "        for i, byte_stream_str in enumerate(dataset['bytes']):\n",
    "            byte_stream = json.loads(byte_stream_str)\n",
    "\n",
    "            processed = proc_bytestream(byte_stream)\n",
    "            assert type(processed) == np.ndarray\n",
    "            assert processed.shape == (256,)\n",
    "            \n",
    "            normalized = on_normalize(processed)\n",
    "            assert normalized.shape == (256,)\n",
    "            assert type(normalized) == np.ndarray\n",
    "\n",
    "            pixels_json = json.dumps(normalized.tolist())\n",
    "            writer.writerow([i, dataset['path'][i], dataset['filename'][i], pixels_json])\n",
    "            export_image(os.path.join(export_directory, f'dataset_{i}.png'), normalized)\n",
    "\n",
    "def get_entire_entropy_converter():\n",
    "    '''\n",
    "    전체 바이트 스트림에 대해 entropy hisrogram 계산 함수 반환\n",
    "    '''\n",
    "    def on_convert(byte_stream:list[int]):\n",
    "        counter = Counter(byte_stream)\n",
    "        bc = [counter[i] for i in range(0, 256)]\n",
    "        return calculate_entropy(bc)\n",
    "    return on_convert\n",
    "\n",
    "def get_sliding_window_entropy_converter(window_size:int=WINDOW_SIZE, step_size:int=STEP_SIZE):\n",
    "    '''\n",
    "    지역 바이트 스트림에 대해 각각 계산후 평균을 내 entropy hisrogram 도출 함수 반환\n",
    "    '''\n",
    "    def on_convert(byte_stream:list[int]):\n",
    "        local_entropies = []\n",
    "        for i in range(0, len(byte_stream), step_size):\n",
    "            counter = Counter(byte_stream[i:i+window_size])\n",
    "            bc = [counter[i] for i in range(0, 256)]\n",
    "            \n",
    "            local_entropies.append(calculate_entropy(bc))\n",
    "        \n",
    "        entropies = np.array(local_entropies)\n",
    "        return np.mean(entropies, axis=0)\n",
    "    return on_convert\n",
    "\n",
    "def get_frequency_converter():\n",
    "    def on_convert(byte_stream:list[int]):\n",
    "        counter = Counter(byte_stream)\n",
    "        frequency = [counter[i] for i in range(0, 256)]\n",
    "        return np.array(frequency)\n",
    "    return on_convert\n",
    "\n",
    "def on_normalize(data:list[int]):\n",
    "    # Min-Max 정규화\n",
    "    normalized = normalize_minmax(data)\n",
    "    # 로그 스케일 변환\n",
    "    normalized = np.log(normalized + 1) / np.log(2) * 255.0\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_PATH = '../export'\n",
    "EXPORT_NORMAL_PATH = os.path.join(EXPORT_PATH, 'normal')\n",
    "EXPORT_ATTACK_PATH = os.path.join(EXPORT_PATH, 'attack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataset_path:list[str], base_export_path:str):\n",
    "    def getpath(*target):\n",
    "        return os.path.join(base_export_path, *target)\n",
    "    for path in dataset_path:\n",
    "        dataset = pd.read_csv(path)\n",
    "        dataset_name , _= os.path.splitext(os.path.split(path)[1])\n",
    "        print(dataset_name)\n",
    "        \n",
    "        print(' - entire entropy')\n",
    "        # 전체 스트림에 대한 entropy histogram\n",
    "        process_dataset(dataset,\n",
    "                        get_entire_entropy_converter(),\n",
    "                        on_normalize=on_normalize,\n",
    "                        export_csvfile=getpath(f'global_entropy_{dataset_name}.csv'),\n",
    "                        export_directory=getpath('global_entropy', dataset_name),\n",
    "                        )\n",
    "        print(' - entropy by sliding')\n",
    "        # 슬라이딩 윈도우를 적용한 entropy histogram\n",
    "        process_dataset(dataset,\n",
    "                        get_sliding_window_entropy_converter(WINDOW_SIZE, STEP_SIZE),\n",
    "                        on_normalize=on_normalize,\n",
    "                        export_csvfile=getpath(f'local_entropy_{dataset_name}.csv'),\n",
    "                        export_directory=getpath('local_entropy', dataset_name),\n",
    "                        )\n",
    "        print(' - frequency')\n",
    "        # 별도의 처리없이 바로 빈도수를 통한 entropy 계산\n",
    "        process_dataset(dataset,\n",
    "                        get_frequency_converter(),\n",
    "                        on_normalize=on_normalize,\n",
    "                        export_csvfile=getpath(f'frequency_{dataset_name}.csv'),\n",
    "                        export_directory=getpath('freqeuncy', dataset_name),\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dataset/attack_decoded.csv', '../dataset/attack_original.csv']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "공격 데이터셋\n",
    "'''\n",
    "dataset_path = []\n",
    "dataset_path.append('../dataset/attack_decoded.csv')\n",
    "dataset_path.append('../dataset/attack_original.csv')\n",
    "\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_decoded\n",
      " - entire entropy\n",
      " - entropy by sliding\n",
      " - frequency\n",
      "attack_original\n",
      " - entire entropy\n",
      " - entropy by sliding\n",
      " - frequency\n"
     ]
    }
   ],
   "source": [
    "run(dataset_path, os.path.join(EXPORT_PATH, 'attack'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dataset/CSV\\\\Github.csv',\n",
       " '../dataset/CSV\\\\GithubGist.csv',\n",
       " '../dataset/CSV\\\\invokeCradleCrafter.csv',\n",
       " '../dataset/CSV\\\\InvokeObfuscation.csv',\n",
       " '../dataset/CSV\\\\IseSteroids.csv',\n",
       " '../dataset/CSV\\\\PoshCode.csv',\n",
       " '../dataset/CSV\\\\PowerShellGallery.csv',\n",
       " '../dataset/CSV\\\\Random.csv',\n",
       " '../dataset/CSV\\\\Technet.csv']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "노멀 데이터셋\n",
    "'''\n",
    "DATASET_BASE_PATH = '../dataset/CSV'\n",
    "dataset_path = []\n",
    "\n",
    "for dirpath, _, filenames in os.walk(DATASET_BASE_PATH):\n",
    "    dataset_path.extend([os.path.join(dirpath, f) for f in filenames])\n",
    "\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_decoded\n",
      " - entire entropy\n",
      " - entropy by sliding\n",
      " - frequency\n",
      "attack_original\n",
      " - entire entropy\n",
      " - entropy by sliding\n",
      " - frequency\n"
     ]
    }
   ],
   "source": [
    "run(dataset_path, os.path.join(EXPORT_PATH, 'normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_byte_histogram_csv(export_path:str):\n",
    "    for dirpath, _, filenames in os.walk(export_path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            if filename.endswith('.csv'):\n",
    "                print('target:', filepath)\n",
    "                try:\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    for jsondata in df['bytes']:\n",
    "                        data = json.loads(jsondata)\n",
    "                        assert len(data) == 256\n",
    "                    print(filepath)\n",
    "                except AssertionError as e:\n",
    "                    print('  Validation failed:', filepath)\n",
    "                    print(e)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error while validating '{filepath}'\")\n",
    "                    print(e)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../export\\attack\\frequency_attack_decoded.csv\n",
      "../export\\attack\\frequency_attack_original.csv\n",
      "../export\\attack\\global_entropy_attack_decoded.csv\n",
      "../export\\attack\\global_entropy_attack_original.csv\n",
      "../export\\attack\\local_entropy_attack_decoded.csv\n",
      "../export\\attack\\local_entropy_attack_original.csv\n",
      "../export\\normal\\frequency_attack_decoded.csv\n",
      "../export\\normal\\frequency_attack_original.csv\n",
      "../export\\normal\\global_entropy_attack_decoded.csv\n",
      "../export\\normal\\global_entropy_attack_original.csv\n",
      "../export\\normal\\local_entropy_attack_decoded.csv\n",
      "../export\\normal\\local_entropy_attack_original.csv\n"
     ]
    }
   ],
   "source": [
    "validate_byte_histogram_csv(EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1) / np.log(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
